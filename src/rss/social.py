"""
ç¤¾äº¤åª’ä½“æŠ“å–æ¨¡å—ï¼ˆTwitter/X + Redditï¼‰
"""
import feedparser
import re
from typing import List, Dict, Optional
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class SocialMediaFetcher:
    """ç¤¾äº¤åª’ä½“æŠ“å–å™¨"""
    
    # RSSHubå¤šå®ä¾‹failover
    RSSHUB_URLS = [
        "https://rsshub.app",
        "https://rsshub.rssforever.com",
    ]
    
    # è‚¡ç¥¨ç›¸å…³Twitterè´¦å·
    TWITTER_ACCOUNTS = [
        "unusual_whales",
        "StockMKTNewz",
        "DeItaone",
        "FirstSquawk",
        "CNBCnow",
    ]
    
    def fetch_twitter_by_symbol(self, symbol: str, max_items: int = 50) -> List[Dict]:
        """
        é€šè¿‡RSSHubæŠ“å–Twitterè‚¡ç¥¨è®¨è®º
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            max_items: æœ€å¤§æ¡ç›®æ•°
            
        Returns:
            List[Dict]: æ¨æ–‡åˆ—è¡¨
        """
        results = []
        cashtag = f"${symbol.upper()}"
        
        for rsshub_url in self.RSSHUB_URLS:
            try:
                url = f"{rsshub_url}/twitter/keyword/{cashtag}"
                feed = feedparser.parse(url)
                
                for entry in feed.entries[:max_items]:
                    results.append({
                        "platform": "twitter",
                        "author": entry.get("author", ""),
                        "content": entry.get("title", ""),
                        "url": entry.get("link", ""),
                        "published": self._parse_date(entry),
                        "symbol": symbol,
                        "likes": 0,
                        "retweets": 0
                    })
                
                if results:
                    break
                    
            except Exception as e:
                logger.warning(f"RSSHub {rsshub_url} failed: {e}")
                continue
        
        return results
    
    def fetch_reddit_by_symbol(self, symbol: str, max_items: int = 50) -> List[Dict]:
        """
        æŠ“å–Redditè‚¡ç¥¨è®¨è®º
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            max_items: æœ€å¤§æ¡ç›®æ•°
            
        Returns:
            List[Dict]: å¸–å­åˆ—è¡¨
        """
        results = []
        subreddits = ["wallstreetbets", "stocks", "investing"]
        
        for subreddit in subreddits:
            try:
                # Redditæœç´¢RSS
                url = f"https://www.reddit.com/r/{subreddit}/search.rss?q={symbol}&restrict_sr=1"
                feed = feedparser.parse(url)
                
                for entry in feed.entries[:max_items]:
                    results.append({
                        "platform": "reddit",
                        "author": entry.get("author", ""),
                        "content": entry.get("title", ""),
                        "url": entry.get("link", ""),
                        "published": self._parse_date(entry),
                        "symbol": symbol,
                        "subreddit": subreddit,
                        "score": 0,
                        "comments": 0
                    })
                    
            except Exception as e:
                logger.warning(f"Reddit r/{subreddit} failed: {e}")
                continue
        
        return results
    
    def fetch_by_symbol(self, symbol: str) -> Dict:
        """
        è·å–è‚¡ç¥¨çš„æ‰€æœ‰ç¤¾äº¤åª’ä½“è®¨è®º
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            
        Returns:
            Dict: åŒ…å«å¸–å­åˆ—è¡¨å’Œæƒ…æ„Ÿç»Ÿè®¡
        """
        twitter_posts = self.fetch_twitter_by_symbol(symbol)
        reddit_posts = self.fetch_reddit_by_symbol(symbol)
        
        all_posts = twitter_posts + reddit_posts
        sentiment = self._analyze_sentiment(all_posts)
        
        return {
            "posts": all_posts,
            "sentiment": sentiment,
            "twitter_count": len(twitter_posts),
            "reddit_count": len(reddit_posts),
            "total": len(all_posts)
        }
    
    def _analyze_sentiment(self, posts: List[Dict]) -> Dict:
        """
        ç®€å•æƒ…æ„Ÿåˆ†æ
        
        Args:
            posts: å¸–å­åˆ—è¡¨
            
        Returns:
            Dict: æƒ…æ„Ÿç»Ÿè®¡
        """
        bullish_keywords = [
            "buy", "long", "bull", "moon", "rocket", "ğŸš€", "ğŸ’°", 
            "calls", "up", "surge", "rally", "breakout"
        ]
        bearish_keywords = [
            "sell", "short", "bear", "crash", "dump", "tank", 
            "puts", "down", "bearish"
        ]
        
        bullish = 0
        bearish = 0
        
        for post in posts:
            text = post.get("content", "").lower()
            b_score = sum(1 for k in bullish_keywords if k in text)
            br_score = sum(1 for k in bearish_keywords if k in text)
            
            if b_score > br_score:
                bullish += 1
            elif br_score > b_score:
                bearish += 1
        
        total = len(posts) if posts else 1
        return {
            "bullish": bullish,
            "bearish": bearish,
            "neutral": len(posts) - bullish - bearish,
            "bullish_pct": round(bullish / total * 100, 1),
            "bearish_pct": round(bearish / total * 100, 1)
        }
    
    def _parse_date(self, entry) -> Optional[datetime]:
        """è§£ææ—¥æœŸ"""
        try:
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                import time
                return datetime.fromtimestamp(time.mktime(entry.published_parsed))
        except:
            pass
        return datetime.now()


# ä¾¿æ·å‡½æ•°
def fetch_social(symbol: str) -> Dict:
    """
    è·å–ç¤¾äº¤åª’ä½“è®¨è®ºçš„ä¾¿æ·å‡½æ•°
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        
    Returns:
        Dict: ç¤¾äº¤åª’ä½“æ•°æ®
    """
    fetcher = SocialMediaFetcher()
    return fetcher.fetch_by_symbol(symbol)
